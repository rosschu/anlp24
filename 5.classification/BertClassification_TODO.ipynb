{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS_Q4bqMNHAn"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/5.classification/BertClassification_TODO.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDh-kfcnxKPI"
   },
   "source": [
    "Thie notebook explores using BERT for text classification.  Before starting, change the runtime to GPU: Runtime > Change runtime type > Hardware accelerator: GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4qAJ8eAN-q6"
   },
   "source": [
    "First, let's download some classification data (feel free to use other data we've worked with this semester)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuPlqorPNVZy"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/dev.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWZy26Ld0nGo"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_N5SRVPyvDt"
   },
   "source": [
    "Double-check that this notebook is running on the GPU (this should \"Running on cuda\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTsJHWfVzS6Y"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH5KcMBMxKPP"
   },
   "outputs": [],
   "source": [
    "def read_labels(filename):\n",
    "    labels={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            if label not in labels:\n",
    "                labels[label]=len(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUOIgxLrxKPQ"
   },
   "outputs": [],
   "source": [
    "def read_data(filename, labels, max_data_points=None):\n",
    "    \"\"\"\n",
    "    :param filename: the name of the file\n",
    "    :return: list of tuple ([word index list], label)\n",
    "    as input for the forward and backward function\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            text = cols[1]\n",
    "\n",
    "            data.append(text)\n",
    "            data_labels.append(labels[label])\n",
    "\n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(data, data_labels))\n",
    "    random.shuffle(tmp)\n",
    "    data, data_labels = zip(*tmp)\n",
    "\n",
    "    if max_data_points is None:\n",
    "        return data, data_labels\n",
    "\n",
    "    return data[:max_data_points], data_labels[:max_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSMOvzrNxKPS"
   },
   "outputs": [],
   "source": [
    "labels=read_labels(\"train.tsv\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvQKP6t2xKPS"
   },
   "source": [
    "We'll limit the training and dev data to 1,000 data points for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmHGHDJDxKPS"
   },
   "outputs": [],
   "source": [
    "train_x, train_y=read_data(\"train.tsv\", labels, max_data_points=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lri3K6TsxKPT"
   },
   "outputs": [],
   "source": [
    "dev_x, dev_y=read_data(\"dev.tsv\", labels, max_data_points=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aMsvhvkxKPT"
   },
   "outputs": [],
   "source": [
    "# Calculates accuracy of input model in test set\n",
    "def evaluate(model, all_x, all_y):\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        idx=0\n",
    "        for x, y in zip(all_x, all_y):\n",
    "\n",
    "            idx+=1\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1\n",
    "    return corr/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERTClassifier is a PyTorch module for text classification using a pre-trained BERT model.\n",
    "    Attributes:\n",
    "        model_name (str): The name of the pre-trained BERT model to use.\n",
    "        tokenizer (BertTokenizer): The tokenizer associated with the pre-trained BERT model.\n",
    "        bert (BertModel): The pre-trained BERT model.\n",
    "        num_labels (int): The number of labels for classification.\n",
    "        fc (nn.Linear): A fully connected layer for classification.\n",
    "    Methods:\n",
    "        get_batches(all_x, all_y, batch_size=32, max_toks=256):\n",
    "            Generates batches of tokenized input data and corresponding labels.\n",
    "        forward(batch_x):\n",
    "            Performs a forward pass through the BERT model and the fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize model name and tokenizer\n",
    "        self.model_name = params[\"model_name\"]\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Number of labels for classification\n",
    "        self.num_labels = params[\"label_length\"]\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
    "\n",
    "    def get_batches(self, all_x, all_y, batch_size=32, max_toks=256):\n",
    "        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer\n",
    "        (and limited to a maximum number of WordPiece tokens) \"\"\"\n",
    "\n",
    "        batches_x = []\n",
    "        batches_y = []\n",
    "\n",
    "        # Iterate over data in batches\n",
    "        for i in range(0, len(all_x), batch_size):\n",
    "            current_batch = []\n",
    "\n",
    "            # Get current batch of data\n",
    "            x = all_x[i:i+batch_size]\n",
    "\n",
    "            # Tokenize the batch of data\n",
    "            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
    "            batch_y = all_y[i:i+batch_size]\n",
    "\n",
    "            # Append tokenized data and labels to batches\n",
    "            batches_x.append(batch_x.to(device))\n",
    "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
    "\n",
    "        return batches_x, batches_y\n",
    "\n",
    "    def forward(self, batch_x):\n",
    "        # Forward pass through BERT model\n",
    "        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
    "                                attention_mask=batch_x[\"attention_mask\"],\n",
    "                                token_type_ids=batch_x[\"token_type_ids\"],\n",
    "                                output_hidden_states=True)\n",
    "\n",
    "        # Get hidden states from BERT output\n",
    "        bert_hidden_states = bert_output['hidden_states']\n",
    "\n",
    "        # Represent the document by its [CLS] embedding (at position 0)\n",
    "        out = bert_hidden_states[-1][:,0,:]\n",
    "\n",
    "        # Pass through fully connected layer for classification\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out  # Return the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC3ysAoFfX4n"
   },
   "source": [
    "Now let's train BERT on this data.  A few practicalities of this environment: if you encounter an out of memory error:\n",
    "\n",
    "* Reset the notebook (Runtime > Factory reset runtime) and execute all cells from the beginning.\n",
    "* If your `max_length` is high, try reducing the `batch_size` in `get_batches` above.\n",
    "\n",
    "Even on a GPU, BERT can take a long time to train, so you might try experimenting first with smaller `max_data_points` above. before running it on the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZtoU7jzxKPU"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a BERT model for classification.\n",
    "\n",
    "    Args:\n",
    "        bert_model_name (str): The name of the pre-trained BERT model to use.\n",
    "        model_filename (str): The filename to save the best model.\n",
    "        train_x (list): Training data features.\n",
    "        train_y (list): Training data labels.\n",
    "        dev_x (list): Development/validation data features.\n",
    "        dev_y (list): Development/validation data labels.\n",
    "        labels (list): List of unique labels in the dataset.\n",
    "        embedding_size (int, optional): Size of the BERT embeddings. Default is 768.\n",
    "        doLowerCase (bool, optional): Whether to lowercase the input text. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize the BERT classifier with the given parameters\n",
    "    bert_model = BERTClassifier(params={\"doLowerCase\": doLowerCase, \"model_name\": bert_model_name, \"embedding_size\": embedding_size, \"label_length\": len(labels)})\n",
    "    \n",
    "    # Move the model to the appropriate device (GPU or CPU)\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # Get batches of training and development data\n",
    "    batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n",
    "    dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n",
    "\n",
    "    # Initialize the optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set the number of epochs and initialize the best development accuracy\n",
    "    num_epochs = 5\n",
    "    best_dev_acc = 0.\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Set PyTorch model to training mode (activates things like dropout and batch normalization)\n",
    "        bert_model.train()\n",
    "\n",
    "        # Train on each batch\n",
    "        for x, y in tqdm(list(zip(batch_x, batch_y))):\n",
    "            y_pred = bert_model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the development set\n",
    "        dev_accuracy = evaluate(bert_model, dev_batch_x, dev_batch_y)\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            if dev_accuracy > best_dev_acc:\n",
    "                torch.save(bert_model.state_dict(), model_filename)\n",
    "                best_dev_acc = dev_accuracy\n",
    "\n",
    "    # Load the best model and print the final results\n",
    "    bert_model.load_state_dict(torch.load(model_filename))\n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
    "    print(\"Time: %.3f seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgLvw1VRiqR_"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(\"bert-base-cased\", \"convote-bert-base-cased\", train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB7gPBAMlWeX"
   },
   "source": [
    "As you can see, training `bert-base` can be expensive.  Google has released a number of [smaller BERT models](https://github.com/google-research/bert) with fewer layers (2, 4, 6, 8, 10) and smaller dimensions (128, 256, 512) that effectively trade off accuracy for speed.  Select a few of these models and train them.  To use these models in the huggingface library that we have been using, the huggingface name of the model can be derived from the URL linking to it:\n",
    "\n",
    "https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip -> `google/bert_uncased_L-2_H-128_A-2`\n",
    "\n",
    "All of these smaller models are uncased (so all text is lowercase), so be sure to set `doLowerCase` to be true.  You'll also need to change the `embedding_size` parameter to this function based on the H value from the model (listed both on the BERT Github page and in the model's URL).  One sample model is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1xQgyJs56En"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(\"google/bert_uncased_L-2_H-128_A-2\", \"lmrd-uncased_L-2_H-128_A-2\", train_x, train_y, dev_x, dev_y, labels, embedding_size=128, doLowerCase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMcEHFz-N1ZD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
