{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inr4HSeKoZbE"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/5.classification/HW6_TransformerClassification_TODO.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubRzhaCVXYUy"
      },
      "source": [
        "Thie notebook explores using transformers for document classification.  Before starting, change the runtime to GPU: Runtime > Change runtime type > Hardware accelerator: GPU (any GPU is fine).\n",
        "\n",
        "For an intro to models in PyTorch, see [this tutorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQnqbL-NjmiP"
      },
      "source": [
        "Download classification data for training/evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9UUcu7TG6sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc45cc12-f324-46b3-c158-d085d4d51ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-08 19:36:08--  https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4660140 (4.4M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]   4.44M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-10-08 19:36:08 (82.2 MB/s) - ‘train.tsv’ saved [4660140/4660140]\n",
            "\n",
            "--2024-10-08 19:36:08--  https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 351382 (343K) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>] 343.15K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-10-08 19:36:09 (11.0 MB/s) - ‘dev.tsv’ saved [351382/351382]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/convote/dev.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwsQCWgmHLrP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS2rympQIObz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a1e2fb-2b02-4293-b7e0-7d4bc9e4faf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDlR2HlLHO7J"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3zDewVZHW23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e166d8-8823-4aaa-bd93-5bb7194bc5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************\n",
            "Running on: cuda\n",
            "********************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# max sequence length\n",
        "max_length=256\n",
        "\n",
        "# limit vocabulary to top N words in training data\n",
        "max_vocab=10000\n",
        "\n",
        "# batch size\n",
        "batch_size=128\n",
        "\n",
        "# size of token representations (which dictates the size of the overall model).\n",
        "d_model=16\n",
        "\n",
        "\n",
        "# number of epochs\n",
        "num_epochs=50\n",
        "\n",
        "print('')\n",
        "print(\"********************************************\")\n",
        "print(\"Running on: {}\".format(device))\n",
        "print(\"********************************************\")\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5IJ2unzHYzu"
      },
      "outputs": [],
      "source": [
        "# PositionalEncoding class copied from:\n",
        "# https://github.com/pytorch/examples/blob/main/word_language_model/model.py\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)#.transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMTmsPDjHast"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_labels, d_model, nhead=2, num_encoder_layers=1, dim_feedforward=256):\n",
        "\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.num_labels=num_labels\n",
        "        self.embedding = nn.Embedding(num_embeddings=max_vocab+2, embedding_dim=d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward, batch_first=True)\n",
        "        self.classifier = nn.Linear(d_model, self.num_labels)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "    def forward(self, x, m):\n",
        "\n",
        "        # put data on device (e.g., gpu)\n",
        "        x=x.to(device)\n",
        "        m=m.to(device)\n",
        "\n",
        "        # convert input token IDs to word embeddings\n",
        "        embed=self.embedding(x)\n",
        "\n",
        "        # add position encodings to include information about word position within the document\n",
        "        embed = self.pos_encoder(embed)\n",
        "\n",
        "        # get transformer output\n",
        "        h=self.transformer.encoder(embed, src_key_padding_mask=m)\n",
        "\n",
        "        # Represent document as average embedding of transformer output\n",
        "        h=torch.mean(h, dim=1)\n",
        "\n",
        "        # Convert document representation into output label space\n",
        "        logits=self.classifier(h)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT56POLcHcLM"
      },
      "outputs": [],
      "source": [
        "def create_vocab_and_labels(filename, max_vocab):\n",
        "    # This function creates the word vocabulary (and label ids) from the training data\n",
        "    # The vocab is a mapping between word types and unique word IDs\n",
        "\n",
        "    counts=Counter()\n",
        "    labels={}\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            lab=cols[0]\n",
        "            text=word_tokenize(cols[1].lower())\n",
        "            for tok in text:\n",
        "                counts[tok]+=1\n",
        "\n",
        "            if lab not in labels:\n",
        "                labels[lab]=len(labels)\n",
        "\n",
        "    vocab={\"[MASK]\":0, \"[UNK]\":1}\n",
        "\n",
        "    for k,v in counts.most_common(max_vocab):\n",
        "        vocab[k]=len(vocab)\n",
        "\n",
        "    return vocab, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB9Vv3TiHdkW"
      },
      "outputs": [],
      "source": [
        "def read_data(filename, vocab, labels, max_length, max_docs=5000):\n",
        "    # Read in data from file, up to the first max_docs documents. For each document\n",
        "    # read up to max_length tokens.\n",
        "\n",
        "    x=[]\n",
        "    y=[]\n",
        "    m=[]\n",
        "\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            if idx >= max_docs:\n",
        "                break\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            lab=cols[0]\n",
        "            text=word_tokenize(cols[1])\n",
        "            text_ids=[]\n",
        "            for tok in text:\n",
        "                if tok in vocab:\n",
        "                    text_ids.append(vocab[tok])\n",
        "                else:\n",
        "                    text_ids.append(vocab[\"[UNK]\"])\n",
        "\n",
        "            text_ids=text_ids[:max_length]\n",
        "\n",
        "            # PyTorch (and most libraries that deal with matrix operations) expects all inputs to be the same length\n",
        "            # So pad each document with 0s up to max_length\n",
        "            # But keep track of the true number of tokens in the document with the \"mask\" list.\n",
        "\n",
        "            # True tokens have a mask value of 0\n",
        "            mask=[0]*len(text_ids)\n",
        "\n",
        "            for i in range(len(text_ids), max_length):\n",
        "                text_ids.append(vocab[\"[MASK]\"])\n",
        "                # Padded tokens have a mask value of 1\n",
        "                mask.append(1)\n",
        "\n",
        "            x.append(text_ids)\n",
        "            m.append(mask)\n",
        "            y.append(labels[lab])\n",
        "\n",
        "    return x, y, m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dglYDfx-HfEt"
      },
      "outputs": [],
      "source": [
        "def get_batches(x, y, m, batch_size):\n",
        "\n",
        "    # Create minibatches from the full dataset\n",
        "\n",
        "    batches_x=[]\n",
        "    batches_y=[]\n",
        "    batches_m=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        xbatch=x[i:i+batch_size]\n",
        "        ybatch=y[i:i+batch_size]\n",
        "        mbatch=m[i:i+batch_size]\n",
        "\n",
        "        batches_x.append(torch.LongTensor(xbatch))\n",
        "        batches_y.append(torch.LongTensor(ybatch))\n",
        "        batches_m.append(torch.BoolTensor(mbatch))\n",
        "\n",
        "    return batches_x, batches_y, batches_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g5m_NjzHgsW"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, all_x, all_y, all_m):\n",
        "\n",
        "    # Calculate accuracy\n",
        "\n",
        "    model.eval()\n",
        "    corr = 0.\n",
        "    total = 0.\n",
        "    with torch.no_grad():\n",
        "        for x, y, m in zip(all_x, all_y, all_m):\n",
        "            y_preds=model.forward(x, m)\n",
        "            for idx, y_pred in enumerate(y_preds):\n",
        "                prediction=torch.argmax(y_pred)\n",
        "                if prediction == y[idx]:\n",
        "                    corr += 1.\n",
        "                total+=1\n",
        "    return corr/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLjdIDl4HiDE"
      },
      "outputs": [],
      "source": [
        "def train(model, model_filename, train_batches_x, train_batches_y, train_batches_m, dev_batches_x, dev_batches_y, dev_batches_m):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "    # Keep track of the epoch that has the best dev accuracy\n",
        "    best_dev_acc=0.\n",
        "    best_dev_epoch=None\n",
        "\n",
        "    # How many epochs with no changes before we quit\n",
        "    patience=10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for x, y, m in zip(train_batches_x, train_batches_y, train_batches_m):\n",
        "            # Get predictions for batch x (with mask values m)\n",
        "            y_pred=model.forward(x, m)\n",
        "            y=y.to(device)\n",
        "\n",
        "            # Calculate loss as cross-entropy with true labels\n",
        "            loss = cross_entropy(y_pred.view(-1, model.num_labels), y.view(-1))\n",
        "\n",
        "            # Set all gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate gradients from current loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y, dev_batches_m)\n",
        "\n",
        "        # we're going to save the model that performs the best on *dev* data\n",
        "        if dev_accuracy > best_dev_acc:\n",
        "            torch.save(model.state_dict(), model_filename)\n",
        "            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n",
        "            best_dev_acc = dev_accuracy\n",
        "            best_dev_epoch=epoch\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "\n",
        "        if epoch-best_dev_epoch > patience:\n",
        "          print(\"%s > patience (%s), stopping...\" % (epoch-best_dev_epoch, patience))\n",
        "          break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filename))\n",
        "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nkNh6f8HkL2"
      },
      "outputs": [],
      "source": [
        "vocab, labels=create_vocab_and_labels(\"train.tsv\", max_vocab)\n",
        "train_x, train_y, train_m=read_data(\"train.tsv\", vocab, labels, max_length=max_length)\n",
        "dev_x, dev_y, dev_m=read_data(\"dev.tsv\", vocab, labels, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vtxV6jpHrAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8c7e52-f85f-4126-9443-f18cbd059b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.494 is better than 0.000, saving model ...\n",
            "Epoch 0, dev accuracy: 0.494\n",
            "Epoch 1, dev accuracy: 0.494\n",
            "0.525 is better than 0.494, saving model ...\n",
            "Epoch 2, dev accuracy: 0.525\n",
            "0.611 is better than 0.525, saving model ...\n",
            "Epoch 3, dev accuracy: 0.611\n",
            "0.646 is better than 0.611, saving model ...\n",
            "Epoch 4, dev accuracy: 0.646\n",
            "Epoch 5, dev accuracy: 0.646\n",
            "Epoch 6, dev accuracy: 0.599\n",
            "Epoch 7, dev accuracy: 0.611\n",
            "Epoch 8, dev accuracy: 0.595\n",
            "Epoch 9, dev accuracy: 0.588\n",
            "Epoch 10, dev accuracy: 0.588\n",
            "Epoch 11, dev accuracy: 0.591\n",
            "Epoch 12, dev accuracy: 0.611\n",
            "0.650 is better than 0.646, saving model ...\n",
            "Epoch 13, dev accuracy: 0.650\n",
            "Epoch 14, dev accuracy: 0.619\n",
            "Epoch 15, dev accuracy: 0.638\n",
            "Epoch 16, dev accuracy: 0.642\n",
            "Epoch 17, dev accuracy: 0.646\n",
            "Epoch 18, dev accuracy: 0.607\n",
            "Epoch 19, dev accuracy: 0.642\n",
            "Epoch 20, dev accuracy: 0.615\n",
            "Epoch 21, dev accuracy: 0.591\n",
            "Epoch 22, dev accuracy: 0.619\n",
            "0.654 is better than 0.650, saving model ...\n",
            "Epoch 23, dev accuracy: 0.654\n",
            "Epoch 24, dev accuracy: 0.595\n",
            "Epoch 25, dev accuracy: 0.576\n",
            "Epoch 26, dev accuracy: 0.591\n",
            "Epoch 27, dev accuracy: 0.615\n",
            "Epoch 28, dev accuracy: 0.588\n",
            "Epoch 29, dev accuracy: 0.599\n",
            "Epoch 30, dev accuracy: 0.595\n",
            "Epoch 31, dev accuracy: 0.603\n",
            "Epoch 32, dev accuracy: 0.599\n",
            "Epoch 33, dev accuracy: 0.638\n",
            "Epoch 34, dev accuracy: 0.626\n",
            "11 > patience (10), stopping...\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4830a4bf0820>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_filename))\n"
          ]
        }
      ],
      "source": [
        "classifier=TransformerClassifier(num_labels=len(labels), d_model=100, dim_feedforward=1024)\n",
        "classifier=classifier.to(device)\n",
        "\n",
        "train_x_batch, train_y_match, train_m_match=get_batches(train_x, train_y, train_m, batch_size=batch_size)\n",
        "dev_x_batch, dev_y_match, dev_m_match=get_batches(dev_x, dev_y, dev_m, batch_size=batch_size)\n",
        "\n",
        "train(classifier, \"test.model\", train_x_batch, train_y_match, train_m_match, dev_x_batch, dev_y_match, dev_m_match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dn2o1KKdz_"
      },
      "source": [
        "**Q1**. Play around with this transformer as implemented and experiment with how performance on the dev data changes as a function of `d_model`, `num_encoder_layers`, `nhead`, etc.).  Describe your experiments and report dev accuracy on them below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tSj2tAMI88r"
      },
      "source": [
        "**Q2**.  This transformer is forced to learn everything about the structure of language from the labeled dataset.  Word embeddings, however, already capture some of this structure, and can be incorporated into this model in an `nn.Embedding` layer.  Change the `TransformerClassifier` class above so that the `Embedding` layer uses pre-trained weights (do so with the `Embedding.from_pretrained` function described on the PyTorch [API](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  You can use any pre-trained embeddings you like, including the [GloVe vectors](https://raw.githubusercontent.com/dbamman/anlp24/main/data/glove.6B.50d.50K.txt) from class.  (Hint: doing so will require changes to `read_data` and `create_vocab_and_labels` since the word embeddings will give you your vocabulary.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiWT6yumKD_l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**.\n",
        "\n",
        "1.  Download a copy of the notebook file: `File > Download > Download .ipynb`.\n",
        "\n",
        "2.  Print the notebook as PDF (via your browser, or tools like [nbconvert](https://nbconvert.readthedocs.io/en/latest/))."
      ],
      "metadata": {
        "id": "mpxBctVxKamS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}