{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/5.classification/HW4_FeatureExploration_TODO.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LJ1_xtISD2WL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZD-F6iB63lv"
      },
      "source": [
        "This notebook explores feature engineering for text classification.  Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function.  What features do you think will help for your particular problem? Your grade is *not* tied to whether accuracy goes up or down, so be creative!  You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajlr1qNk63lx"
      },
      "source": [
        "You are free to use any of the following datasets for this exercise, or to use your own (if you have your own labeled data with at least 500 examples from at least two classes, I would encourage you to use it!).  If you use your own data, just be sure to format it like the examples below; each directory has a `train.tsv`, `dev.tsv` and `test.tsv` file, where each file is tab-separated (label in the first column and text in the second column).\n",
        "\n",
        "* [Sentiment Analysis](https://ai.stanford.edu/~amaas/data/sentiment/) (Positive/Negative)\n",
        "* [Congressional Speech](https://www.cs.cornell.edu/home/llee/data/convote.html) (Democrat/Republican)\n",
        "* Library of Congress Subject Classication ([21 categories](https://en.wikipedia.org/wiki/Library_of_Congress_Classification))\n",
        "\n",
        "For whichever dataset you pick, download the data first using the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get LMRD data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/train.tsv -O lmrd_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/dev.tsv -O lmrd_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/test.tsv -O lmrd_test.tsv"
      ],
      "metadata": {
        "id": "HNh-QjKm67VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get Convote data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/train.tsv -O convote_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/dev.tsv -O convote_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/test.tsv -O convote_test.tsv"
      ],
      "metadata": {
        "id": "fJd1C1cu7QD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get LoC data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/train.tsv -O loc_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/dev.tsv -O loc_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/test.tsv -O loc_test.tsv"
      ],
      "metadata": {
        "id": "y2kamLx07T6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaiV9Dy763lx"
      },
      "source": [
        "**Q0**: Briefly describe your data (including the categories you're predicting).  If you're using your own data, tell us about it; if you're using one of the datasets above, tell us something that shows you've looked at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh28crHR63ly"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import Counter\n",
        "import operator\n",
        "from sklearn import preprocessing\n",
        "from sklearn import linear_model\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kDz__uf63lz"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            label=cols[0]\n",
        "            text=word_tokenize(cols[1])\n",
        "            X.append(text)\n",
        "            Y.append(label)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej8Ge3bR63lz"
      },
      "outputs": [],
      "source": [
        "# Change this to the directory with the data you will be using.\n",
        "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
        "data=\"loc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KU2zqTM63lz"
      },
      "outputs": [],
      "source": [
        "trainX, trainY=read_data(\"%s_train.tsv\" % data)\n",
        "devX, devY=read_data(\"%s_dev.tsv\" % data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l-v6WCl63l0"
      },
      "outputs": [],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority=labelCounts.most_common(1)[0][0]\n",
        "\n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority:\n",
        "            correct+=1\n",
        "\n",
        "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPtsXae_63l0"
      },
      "source": [
        "Here we'll create two feature classes -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7wpOrJ563l1"
      },
      "outputs": [],
      "source": [
        "# Here's a sample dictionary if we were using the convote political data\n",
        "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
        "repub_dictionary=set([\"growth\",\"economy\"])\n",
        "\n",
        "def political_dictionary_feature(tokens):\n",
        "    feats={}\n",
        "    for word in tokens:\n",
        "        if word in dem_dictionary:\n",
        "            feats[\"word_in_dem_dictionary\"]=1\n",
        "        if word in repub_dictionary:\n",
        "            feats[\"word_in_repub_dictionary\"]=1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtO-foMj63l1"
      },
      "outputs": [],
      "source": [
        "def unigram_feature(tokens):\n",
        "    feats={}\n",
        "    for word in tokens:\n",
        "        feats[\"UNIGRAM_%s\" % word]=1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7yZfUkU63l1"
      },
      "source": [
        "**Q1**: Add first new feature function here.  Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAw7PO_w63l1"
      },
      "outputs": [],
      "source": [
        "def new_feature_class_one(tokens):\n",
        "    feats={}\n",
        "    feats[\"_FILL_IN_FEATURES_HERE_\"]=1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GLI_RtJ63l2"
      },
      "source": [
        "**Q2**: Add second new feature function here. Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNcqQIx863l2"
      },
      "outputs": [],
      "source": [
        "def new_feature_class_two(tokens):\n",
        "    feats={}\n",
        "    feats[\"_FILL_IN_FEATURES_HERE_\"]=1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07dK4NPp63l2"
      },
      "source": [
        "This is the main function we'll use to aggregate together all of the information from different feature classes.  Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning.  (Here you want to make sure that the keys each feature function is creating are unique so they don't get clobbered by other functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr3aD3QF63l2"
      },
      "outputs": [],
      "source": [
        "def build_features(trainX, feature_functions):\n",
        "    data=[]\n",
        "    for tokens in trainX:\n",
        "        feats={}\n",
        "\n",
        "        for function in feature_functions:\n",
        "            feats.update(function(tokens))\n",
        "\n",
        "        data.append(feats)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuuPpvmS63l2"
      },
      "outputs": [],
      "source": [
        "# This helper function converts a dictionary of feature names to unique numerical ids\n",
        "def create_vocab(data):\n",
        "    feature_vocab={}\n",
        "    idx=0\n",
        "    for doc in data:\n",
        "        for feat in doc:\n",
        "            if feat not in feature_vocab:\n",
        "                feature_vocab[feat]=idx\n",
        "                idx+=1\n",
        "\n",
        "    return feature_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOuDxyx63l2"
      },
      "outputs": [],
      "source": [
        "# This helper function converts a dictionary of feature names to a sparse representation\n",
        "# that we can fit in a scikit-learn model.  This is important because almost all feature\n",
        "# values will be 0 for most documents (note: why?), and we don't want to save them all in\n",
        "# memory.\n",
        "\n",
        "def features_to_ids(data, feature_vocab):\n",
        "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
        "    for idx,doc in enumerate(data):\n",
        "        for f in doc:\n",
        "            if f in feature_vocab:\n",
        "                new_data[idx,feature_vocab[f]]=doc[f]\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMh8GzFm63l2"
      },
      "outputs": [],
      "source": [
        "# This function evaluates a list of feature functions on the training/dev data arguments\n",
        "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
        "    trainX_feat=build_features(trainX, feature_functions)\n",
        "    devX_feat=build_features(devX, feature_functions)\n",
        "\n",
        "    # just create vocabulary from features in *training* data\n",
        "    feature_vocab=create_vocab(trainX_feat)\n",
        "\n",
        "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
        "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
        "\n",
        "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
        "    logreg.fit(trainX_ids, trainY)\n",
        "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))\n",
        "    return logreg, feature_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH7bPWTA63l3"
      },
      "outputs": [],
      "source": [
        "def print_weights(clf, vocab, n=10):\n",
        "\n",
        "    reverse_vocab=[None]*len(clf.coef_[0])\n",
        "    for k in vocab:\n",
        "        reverse_vocab[vocab[k]]=k\n",
        "\n",
        "    if len(clf.classes_) == 2:\n",
        "\n",
        "        weights=clf.coef_[0]\n",
        "        for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "        print()\n",
        "\n",
        "        for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "    else:\n",
        "        for i, cat in enumerate(clf.classes_):\n",
        "\n",
        "            weights=clf.coef_[i]\n",
        "\n",
        "            for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFfW35rB63l3"
      },
      "outputs": [],
      "source": [
        "majority_class(trainY,devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9jWjaT63l3"
      },
      "source": [
        "Explore the impact of different feature functions by evaluating them below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ci2NKi63l3"
      },
      "outputs": [],
      "source": [
        "features=[unigram_feature]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsGfRng63l3"
      },
      "source": [
        "If you want to print the coefficients for any of the models you train, you can do so like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzzj7s4Q63l3"
      },
      "outputs": [],
      "source": [
        "print_weights(clf, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDKnZ8V263l3"
      },
      "outputs": [],
      "source": [
        "features=[political_dictionary_feature, unigram_feature]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K65GmQk463l3"
      },
      "outputs": [],
      "source": [
        "features=[new_feature_class_one]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UphUTfr-63l4"
      },
      "outputs": [],
      "source": [
        "features=[new_feature_class_two]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLR6EMn763l4"
      },
      "outputs": [],
      "source": [
        "features=[new_feature_class_one, new_feature_class_two]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im9RNrpo63l4"
      },
      "outputs": [],
      "source": [
        "features=[unigram_feature, new_feature_class_one, new_feature_class_two]\n",
        "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVwYUGla63l4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**.\n",
        "\n",
        "1.  Download a copy of the notebook file: `File > Download > Download .ipynb`.\n",
        "\n",
        "2.  Print the notebook as PDF (via your browser, or tools like [nbconvert](https://nbconvert.readthedocs.io/en/latest/))."
      ],
      "metadata": {
        "id": "lFfAJs8FEDc-"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}