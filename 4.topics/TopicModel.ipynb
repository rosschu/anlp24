{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/4.topics/TopicModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9IMbBbn2vK1"
   },
   "source": [
    "In this notebook we'll explore topic modeling to discover broad themes in a collection of movie summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiJYciPG2vK4",
    "outputId": "f6b7fb84-2c15-4a0e-e687-c034c54ba8dd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZF3Si83290-",
    "outputId": "10237cff-9a10-45af-8ad1-3ef8e7b925f5"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/jockers.stopwords\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/movie.metadata.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/plot_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWKhujgi2vK6"
   },
   "outputs": [],
   "source": [
    "def read_stopwords(filename):\n",
    "    stopwords={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            stopwords[line.rstrip()]=1\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1phZTyi2vK6"
   },
   "source": [
    "Since we're running topic modeling on texts with lots of names, we'll add the Jockers list of stopwords (which includes character names) to our stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8-0-X_-2vK7"
   },
   "outputs": [],
   "source": [
    "stop_words = {k:1 for k in stopwords.words('english')}\n",
    "stop_words.update(read_stopwords(\"jockers.stopwords\"))\n",
    "stop_words[\"'s\"]=1\n",
    "stop_words=list(stop_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAMInj_R2vK7"
   },
   "outputs": [],
   "source": [
    "def filter(word, stopwords):\n",
    "\n",
    "    \"\"\" Function to exclude words from a text \"\"\"\n",
    "\n",
    "    # no stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "\n",
    "    # has to contain at least one letter\n",
    "    if re.search(\"[A-Za-z]\", word) is not None:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIxxxBWu2vK7"
   },
   "outputs": [],
   "source": [
    "def read_docs(plotFile, metadataFile, stopwords):\n",
    "\n",
    "    names={}\n",
    "    box={}\n",
    "\n",
    "    with open(metadataFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            name=cols[2]\n",
    "            boxoffice=cols[4]\n",
    "            if len(boxoffice) != 0:\n",
    "                box[idd]=int(boxoffice)\n",
    "                names[idd]=name\n",
    "\n",
    "    n=5000\n",
    "    target_movies={}\n",
    "\n",
    "\n",
    "    sorted_box = sorted(box.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_box[:n]:\n",
    "        target_movies[k]=names[k]\n",
    "\n",
    "    docs=[]\n",
    "    names=[]\n",
    "\n",
    "    with open(plotFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            text=cols[1]\n",
    "\n",
    "            if idd in target_movies:\n",
    "                tokens=nltk.word_tokenize(text.lower())\n",
    "                tokens=[x for x in tokens if filter(x, stopwords)]\n",
    "                docs.append(tokens)\n",
    "                name=target_movies[idd]\n",
    "                names.append(name)\n",
    "    return docs, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AF6QANy2vK8"
   },
   "source": [
    "We'll read in summaries of the 5,000 movies with the highest box office revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaYqSs8F2vK8"
   },
   "outputs": [],
   "source": [
    "metadataFile=\"movie.metadata.tsv\"\n",
    "plotFile=\"plot_summaries.txt\"\n",
    "data, doc_names=read_docs(plotFile, metadataFile, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQUWExT22vK9"
   },
   "source": [
    "We will convert the movie summaries into a bag-of-words representation using gensim's [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0zxKTmJ2vK9"
   },
   "outputs": [],
   "source": [
    "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents\n",
    "# and no more than 50% of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(data)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAAsrxaI2vK9"
   },
   "outputs": [],
   "source": [
    "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hE64x_22vK9"
   },
   "outputs": [],
   "source": [
    "num_topics=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJqjhoFL2vK-"
   },
   "source": [
    "Now let's run a topic model on this data using gensim's built-in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "373pP7-02vK-"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUduxAS92vK-"
   },
   "source": [
    "We can get a sense of what the topics are by printing the top 10 words with highest $P(word \\mid topic)$ for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pG3HiUOC2vK-",
    "outputId": "cfb76fd4-756e-4cbf-e775-8b3cfe1556ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yirq9huh2vK-"
   },
   "source": [
    "Another way of understanding topics is to print out the documents that have the highest topic representation -- i.e., for a given topic $k$, the documents with highest $P(topic=k | document)$.  How much do the documents listed here align with your understanding of the topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDrsC_zz2vK-",
    "outputId": "f61bcab5-8826-4779-8c97-58f8e0bf4c8c"
   },
   "outputs": [],
   "source": [
    "topic_model=lda_model\n",
    "\n",
    "topic_docs=[]\n",
    "for i in range(num_topics):\n",
    "    topic_docs.append({})\n",
    "for doc_id in range(len(corpus)):\n",
    "    doc_topics=topic_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_num, topic_prob in doc_topics:\n",
    "        topic_docs[topic_num][doc_id]=topic_prob\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print(\"%s\\n\" % ' '.join([term for term, freq in topic_model.show_topic(i, topn=10)]))\n",
    "    sorted_x = sorted(topic_docs[i].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_x[:5]:\n",
    "        print(\"%s\\t%.3f\\t%s\" % (i,v,doc_names[k]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1k2yMu2vK-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
