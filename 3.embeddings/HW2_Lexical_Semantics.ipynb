{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHCHnheBcwYs"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/3.embeddings/HW2_Lexical_Semantics.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework 2: Lexical Semantics"
      ],
      "metadata": {
        "id": "tTNSdu8qV1lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will explore lexical semantics in the context of slang and FastText, an alternative to word2vec (Part 1); and, how to represent a sentence with individual word vectors, so we can measure the similarity between a pair of sentences (Part 2)."
      ],
      "metadata": {
        "id": "NDmHpjTJziCs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZ1YG3DZ0aKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5qAYY2vxLH"
      },
      "source": [
        "### Part 1: Slang and word similarity with FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slang presents an interesting linguistic phenomenon that involves non-standard word forms. For this question, you will explore how FastText, an alternative to Word2Vec, handles the lexical semantics of slang and informal language.\n",
        "\n",
        "First, familiarize yourself with the slang dataset that we are using, introduced in [\"Toward Informal Language Processing: Knowledge of Slang in Large Language Models\" (Sun et al., NAACL 2024)](https://aclanthology.org/2024.naacl-long.94/). The full dataset includes annotations indicating whether a sentence from OpenSubtitles (typically a line from a movie) contains a slang term, and you can find some example sentences and terms here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp24/main/data/slang_examples.tsv\n",
        "\n",
        "Next, train a FastText model using `gensim` and `FastText` on our slang data derived from the dataset described above, which you can download here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp24/main/data/slang_corpus.txt\n",
        "\n",
        "For preprocessing, in the `txt` file, each token is already separated by whitespace, so you don't need to worry about tokenization. For training, use the following parameters: embedding size of 400, context window of 5, frequency threshold of 5, and use 5 workers to train for 5 epochs.\n",
        "\n",
        "With the trained models:\n",
        "\n",
        "**Q1.** Pick a slang term, and in about 100 words, discuss:\n",
        "\n",
        "- what the most similar words are to the slang term of your choosing (as measured by the model), and\n",
        "- whether the result is aligned with your understanding."
      ],
      "metadata": {
        "id": "UpqZQI31YG-S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9ciSWaFtIhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.** Train a separate word2vec (not FastText) model using the same dataset, and in about 100 words, compare the two approaches used to esitmate word vectors. Here are some potential topics:\n",
        "\n",
        "- Look up the token `gonna` in both word2vec and FastText models. What does this tell you?\n",
        "- What is the high level difference between word2vec and FastText?\n",
        "- Evaluate the quality of the trained embeddings through intrinsic evaluation."
      ],
      "metadata": {
        "id": "Fy1QSpMktM_W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7KP9ey8tNd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2. From words to sentences\n",
        "\n",
        "So far we've been working with word vectors, but in real-world scenarios, we often want to work with not just a word, but a sequence (like a sentence), which will explore later in the semester. However, with what we have learned so far, how do you represent a sentence? One approach is to look up the word vectors for individual words in the sentence and then *average* them, which we will explore in this question. We will be using pre-trained GloVe vectors [cf. SLP 6.8.3] we used in class. Download them here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp24/main/data/glove.6B.100d.100K.w2v.txt\n",
        "\n",
        "**Q3.** Load the pretrained embeddings with `gensim`'s `load_word2vec_format`, and create a function that takes a pair of sentences as input, and outputs the similarity of the two sentences measured by cosine -- the sentence pair you can use for sanity check is provided below. You will be evaluated primarily on your understanding of the problem and relevant concepts covered in class."
      ],
      "metadata": {
        "id": "GtbVFoKquMFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check: compare the similarity between:\n",
        "#       sentences[0], sentences[1]\n",
        "#       sentences[0], sentences[2]\n",
        "\n",
        "sentences = [\n",
        "    ['the', 'cat', 'is', 'on', 'the', 'mat'],\n",
        "    ['a', 'computer', 'is', 'in', 'the', 'store'],\n",
        "    ['the', 'cat', 'sits', 'on', 'the', 'mat']\n",
        "]"
      ],
      "metadata": {
        "id": "fFArdN1mMSOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYiKUIr5cwYw"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**.\n",
        "\n",
        "1.  Download a copy of the notebook file: `File > Download > Download .ipynb`.\n",
        "\n",
        "2.  Print the notebook as PDF. If you're experiencing difficulty printing the PDF file, consider using the helper below."
      ],
      "metadata": {
        "id": "WqWFmNaBPYUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF export helper"
      ],
      "metadata": {
        "id": "_TfNi4MzsTLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoyANORycwYw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#EXPORT_EXCLUDE#\n",
        "\n",
        "\n",
        "#@markdown This runs `nbconvert` in the background:\n",
        "\n",
        "\"\"\"\n",
        "Based on: https://gist.github.com/cjgunnar/f1e05a586cd3b8858b077bb9ef4897f0\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive, files\n",
        "from pathlib import Path\n",
        "\n",
        "# write changes then mount\n",
        "drive.flush_and_unmount()\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#@markdown 1. Enter the path of this notebook in your Google Drive. By default, it is saved in the `Colab Notebooks` folder. To confirm, go to the `File` menu and click `Locate in Drive`.\n",
        "\n",
        "path = \"Colab Notebooks/HW2_Lexical_Semantics.ipynb\" #@param {type: \"string\"}\n",
        "path = f\"/content/drive/MyDrive/{path}\"\n",
        "\n",
        "#@markdown 2. Run this cell to produce the PDF file (you will need to give Colab permission to access your Drive). This will take a while (~2 mins). Save the PDF file when prompted.\n",
        "\n",
        "path = Path(path)\n",
        "\n",
        "if not path.exists():\n",
        "  print(\"No file found at \", path)\n",
        "  raise FileNotFoundError()\n",
        "\n",
        "print(\"Installing dependencies ...\")\n",
        "\n",
        "# install dependencies... this will take a while\n",
        "!sudo apt -q install -y gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wget &> /dev/null\n",
        "!python -m pip -qqq install -U notebook-as-pdf &> /dev/null\n",
        "!pip install -qqq nbconvert[webpdf] -U &> /dev/null\n",
        "!pyppeteer-install &> /dev/null\n",
        "!playwright install &> /dev/null\n",
        "!sudo apt -q install chromium-chromedriver &> /dev/null\n",
        "\n",
        "# credit to https://stackoverflow.com/questions/57217924/pyppeteer-errors-browsererror-browser-closed-unexpectedly\n",
        "!jupyter nbconvert --disable-chromium-sandbox --WebPDFExporter.disable_sandbox=True --RegexRemovePreprocessor.patterns=\"^#EXPORT_EXCLUDE#.*\" --no-input --to webpdf \"{path}\"\n",
        "\n",
        "output = path.with_suffix('.pdf')\n",
        "files.download(output)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_TfNi4MzsTLk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}