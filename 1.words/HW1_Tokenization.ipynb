{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/1.words/HW1_Tokenization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 Submission\n",
    "## Ross Chu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv5qAYY2vxLH"
   },
   "source": [
    "In this homework, you'll compare the different tokenizations that result from different clases of tokenizers.  This homework is also for you to check in yourself on your Python proficiency; for all of the operations below (downloading a file, reading it in, counting objects), you should either be comfortable implementing them already or know how to find out how to do so yourself (if you find yourself struggling with them, we encourage you to take this class at a later date, with more Python experience under your belt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uzG_NzfvK60"
   },
   "source": [
    "Q1. Tokenize the following document with each of these models. Feel free to use the documentation linked (and AI Assistance) to do so for this low-level operation (but again remember that you have to be able to explain what your code is doing).  For each of the tokenizers above, we want to see a list of tokens for this document (not numeric token IDs, but legible words) -- e.g., \\[\"London\", \".\", ...\\]\n",
    "\n",
    "* NLTK `word_tokenize` (https://www.nltk.org/book/ch03.html)\n",
    "* Spacy `tokenize` (https://spacy.io/usage/spacy-101#annotations-token)\n",
    "* Tiktoken BPE tokenization (https://github.com/openai/tiktoken) -- cl100k_base (GPT-3.5, GPT-4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1725398235106,
     "user": {
      "displayName": "David Alexander Bamman",
      "userId": "09859182337762586004"
     },
     "user_tz": 420
    },
    "id": "9ywLVgL3v5sy"
   },
   "outputs": [],
   "source": [
    "# Doc string\n",
    "document=\"London. Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln’s Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth, and it would not be wonderful to meet a Megalosaurus, forty feet long or so, waddling like an elephantine lizard up Holborn Hill. Smoke lowering down from chimney-pots, making a soft black drizzle, with flakes of soot in it as big as full-grown snowflakes—gone into mourning, one might imagine, for the death of the sun. Dogs, undistinguishable in mire. Horses, scarcely better; splashed to their very blinkers. Foot passengers, jostling one another’s umbrellas in a general infection of ill temper, and losing their foot-hold at street-corners, where tens of thousands of other foot passengers have been slipping and sliding since the day broke (if this day ever broke), adding new deposits to the crust upon crust of mud, sticking at those points tenaciously to the pavement, and accumulating at compound interest.\"\n",
    "\n",
    "# Initialize dictionary to store tokens\n",
    "tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tokenizer\n",
    "# Reference: https://www.nltk.org/book/ch03.html\n",
    "tokens['nltk'] = nltk.word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenizer\n",
    "# Reference: https://spacy.io/usage/spacy-101#annotations-token\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokens['spacy'] = [token.text for token in nlp(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiktoken BPE tokenizer\n",
    "# Reference: https://github.com/openai/tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens['bpe'] = [enc.decode([token_id]) for token_id in enc.encode(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk tokenizer:\n",
      "['London', '.', 'Michaelmas', 'term', 'lately', 'over', ',', 'and', 'the', 'Lord', 'Chancellor', 'sitting', 'in', 'Lincoln', '’', 's', 'Inn', 'Hall', '.', 'Implacable', 'November', 'weather', '.', 'As', 'much', 'mud', 'in', 'the', 'streets', 'as', 'if', 'the', 'waters', 'had', 'but', 'newly', 'retired', 'from', 'the', 'face', 'of', 'the', 'earth', ',', 'and', 'it', 'would', 'not', 'be', 'wonderful', 'to', 'meet', 'a', 'Megalosaurus', ',', 'forty', 'feet', 'long', 'or', 'so', ',', 'waddling', 'like', 'an', 'elephantine', 'lizard', 'up', 'Holborn', 'Hill', '.', 'Smoke', 'lowering', 'down', 'from', 'chimney-pots', ',', 'making', 'a', 'soft', 'black', 'drizzle', ',', 'with', 'flakes', 'of', 'soot', 'in', 'it', 'as', 'big', 'as', 'full-grown', 'snowflakes—gone', 'into', 'mourning', ',', 'one', 'might', 'imagine', ',', 'for', 'the', 'death', 'of', 'the', 'sun', '.', 'Dogs', ',', 'undistinguishable', 'in', 'mire', '.', 'Horses', ',', 'scarcely', 'better', ';', 'splashed', 'to', 'their', 'very', 'blinkers', '.', 'Foot', 'passengers', ',', 'jostling', 'one', 'another', '’', 's', 'umbrellas', 'in', 'a', 'general', 'infection', 'of', 'ill', 'temper', ',', 'and', 'losing', 'their', 'foot-hold', 'at', 'street-corners', ',', 'where', 'tens', 'of', 'thousands', 'of', 'other', 'foot', 'passengers', 'have', 'been', 'slipping', 'and', 'sliding', 'since', 'the', 'day', 'broke', '(', 'if', 'this', 'day', 'ever', 'broke', ')', ',', 'adding', 'new', 'deposits', 'to', 'the', 'crust', 'upon', 'crust', 'of', 'mud', ',', 'sticking', 'at', 'those', 'points', 'tenaciously', 'to', 'the', 'pavement', ',', 'and', 'accumulating', 'at', 'compound', 'interest', '.']\n",
      "\n",
      "\n",
      "spacy tokenizer:\n",
      "['London', '.', 'Michaelmas', 'term', 'lately', 'over', ',', 'and', 'the', 'Lord', 'Chancellor', 'sitting', 'in', 'Lincoln', '’s', 'Inn', 'Hall', '.', 'Implacable', 'November', 'weather', '.', 'As', 'much', 'mud', 'in', 'the', 'streets', 'as', 'if', 'the', 'waters', 'had', 'but', 'newly', 'retired', 'from', 'the', 'face', 'of', 'the', 'earth', ',', 'and', 'it', 'would', 'not', 'be', 'wonderful', 'to', 'meet', 'a', 'Megalosaurus', ',', 'forty', 'feet', 'long', 'or', 'so', ',', 'waddling', 'like', 'an', 'elephantine', 'lizard', 'up', 'Holborn', 'Hill', '.', 'Smoke', 'lowering', 'down', 'from', 'chimney', '-', 'pots', ',', 'making', 'a', 'soft', 'black', 'drizzle', ',', 'with', 'flakes', 'of', 'soot', 'in', 'it', 'as', 'big', 'as', 'full', '-', 'grown', 'snowflakes', '—', 'gone', 'into', 'mourning', ',', 'one', 'might', 'imagine', ',', 'for', 'the', 'death', 'of', 'the', 'sun', '.', 'Dogs', ',', 'undistinguishable', 'in', 'mire', '.', 'Horses', ',', 'scarcely', 'better', ';', 'splashed', 'to', 'their', 'very', 'blinkers', '.', 'Foot', 'passengers', ',', 'jostling', 'one', 'another', '’s', 'umbrellas', 'in', 'a', 'general', 'infection', 'of', 'ill', 'temper', ',', 'and', 'losing', 'their', 'foot', '-', 'hold', 'at', 'street', '-', 'corners', ',', 'where', 'tens', 'of', 'thousands', 'of', 'other', 'foot', 'passengers', 'have', 'been', 'slipping', 'and', 'sliding', 'since', 'the', 'day', 'broke', '(', 'if', 'this', 'day', 'ever', 'broke', ')', ',', 'adding', 'new', 'deposits', 'to', 'the', 'crust', 'upon', 'crust', 'of', 'mud', ',', 'sticking', 'at', 'those', 'points', 'tenaciously', 'to', 'the', 'pavement', ',', 'and', 'accumulating', 'at', 'compound', 'interest', '.']\n",
      "\n",
      "\n",
      "bpe tokenizer:\n",
      "['London', '.', ' Michael', 'mas', ' term', ' lately', ' over', ',', ' and', ' the', ' Lord', ' Chancellor', ' sitting', ' in', ' Lincoln', '’s', ' Inn', ' Hall', '.', ' Impl', 'ac', 'able', ' November', ' weather', '.', ' As', ' much', ' mud', ' in', ' the', ' streets', ' as', ' if', ' the', ' waters', ' had', ' but', ' newly', ' retired', ' from', ' the', ' face', ' of', ' the', ' earth', ',', ' and', ' it', ' would', ' not', ' be', ' wonderful', ' to', ' meet', ' a', ' Meg', 'al', 'os', 'aurus', ',', ' forty', ' feet', ' long', ' or', ' so', ',', ' w', 'add', 'ling', ' like', ' an', ' elephant', 'ine', ' lizard', ' up', ' Hol', 'born', ' Hill', '.', ' Smoke', ' lowering', ' down', ' from', ' chimney', '-p', 'ots', ',', ' making', ' a', ' soft', ' black', ' dr', 'izzle', ',', ' with', ' flakes', ' of', ' so', 'ot', ' in', ' it', ' as', ' big', ' as', ' full', '-g', 'rown', ' snow', 'fl', 'akes', '—', 'gone', ' into', ' mourning', ',', ' one', ' might', ' imagine', ',', ' for', ' the', ' death', ' of', ' the', ' sun', '.', ' Dogs', ',', ' und', 'istinguish', 'able', ' in', ' m', 'ire', '.', ' H', 'orses', ',', ' scarcely', ' better', ';', ' spl', 'ashed', ' to', ' their', ' very', ' blink', 'ers', '.', ' Foot', ' passengers', ',', ' j', 'ost', 'ling', ' one', ' another', '’s', ' umb', 'rellas', ' in', ' a', ' general', ' infection', ' of', ' ill', ' temper', ',', ' and', ' losing', ' their', ' foot', '-h', 'old', ' at', ' street', '-c', 'orners', ',', ' where', ' tens', ' of', ' thousands', ' of', ' other', ' foot', ' passengers', ' have', ' been', ' slipping', ' and', ' sliding', ' since', ' the', ' day', ' broke', ' (', 'if', ' this', ' day', ' ever', ' broke', '),', ' adding', ' new', ' deposits', ' to', ' the', ' crust', ' upon', ' crust', ' of', ' mud', ',', ' sticking', ' at', ' those', ' points', ' ten', 'ac', 'iously', ' to', ' the', ' pavement', ',', ' and', ' accumulating', ' at', ' compound', ' interest', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print tokens for each tokenizer\n",
    "for key in tokens.keys():\n",
    "    print(f\"{key} tokenizer:\")\n",
    "    print(tokens[key])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbhjqG9m05yS"
   },
   "source": [
    "Q2. Examine the different tokenizations for the passage above -- i.e., actually read through them and see how they differ. In a paragraph or two, characterize the salient differences in tokenization between a.) NLTK and Spacy and b.) NLTK and BPE.  Reference real examples in the text.  (At the end of this homework, you want to be able to discuss the practical differences between tokenization methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Response:\n",
    "\n",
    "### NLTK vs Spacy\n",
    "NLTK seems better than Spacy at capturing words that contain punctuation. For example, full-grown, street-corners, and foot-hold are treated as single tokens in NLTK while they are separated by dashes for Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK vs BPE\n",
    "BPE splits words into its subwords produced by NLTK. Doing so increases the dimensionality of vocabulary by breaking down each word into many sub words. Examples include implacable VS impl + ac + able, megalosaurus VS meg + al + os + aurus, weddling VS w + add + ling, and elephantine VS elephant + ine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwhNqD61uZi4"
   },
   "source": [
    "Q3. Download the full text of *Pride and Prejudice* (https://raw.githubusercontent.com/dbamman/anlp24/main/data/1342_pride_and_prejudice.txt) and tokenize it using each of the methods above. How many word types (in the formal sense we discussed in class) does each tokenization method have for that complete file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Response: \n",
    "NLTK: 7475 types \\\n",
    "Spacy: 6780 types \\\n",
    "BPE: 8364 types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc string\n",
    "file_path='/Users/RossChu/GoogleDrive/ANLP2024/anlp24/data/1342_pride_and_prejudice.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    document = file.read()\n",
    "\n",
    "# Set tokenizers for spacy and tiktoken\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Tokenize doc with each tokenizer\n",
    "tokens = {}\n",
    "tokens['nltk'] = nltk.word_tokenize(document)\n",
    "tokens['spacy'] = [token.text for token in nlp(document)]\n",
    "tokens['bpe'] = [enc.decode([token_id]) for token_id in enc.encode(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nltk': 7475, 'spacy': 6780, 'bpe': 8364}\n"
     ]
    }
   ],
   "source": [
    "# Count word types for each tokenization method\n",
    "types = {key: len(set(tokens[key])) for key in tokens.keys()}\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vf1sRWp3LOb"
   },
   "source": [
    "Q4. Which text has the greater type-token ratio, *Pride and Prejudice* (https://raw.githubusercontent.com/dbamman/anlp24/main/data/1342_pride_and_prejudice.txt) or *Emma* (https://raw.githubusercontent.com/dbamman/anlp24/main/data/158_emma.txt)?  Calculate the TTR for both texts using the NLTK tokenizer, but only use the first 1,000 tokens from each text when calculating its TTR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Response:\n",
    "\n",
    "Emma has more types (410) compared with pride and prejudice (360), resulting in a lower TTR for emma (2.44) compared with pride and prejudice (2.78). This suggests that Emma uses richer vocabulary than pride and prejudice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3fn4C66L3lvZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1342_pride_and_prejudice': 2.78, '158_emma': 2.44}\n",
      "{'1342_pride_and_prejudice': 1000, '158_emma': 1000}\n",
      "{'1342_pride_and_prejudice': 360, '158_emma': 410}\n"
     ]
    }
   ],
   "source": [
    "# Doc strings for two texts\n",
    "doc = {}\n",
    "for doc_file in ['1342_pride_and_prejudice','158_emma']:\n",
    "    file_path=f'/Users/RossChu/GoogleDrive/ANLP2024/anlp24/data/{doc_file}.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        doc[doc_file] = file.read()\n",
    "\n",
    "# Get first 1000 tokens from each doc\n",
    "tokens = {key: nltk.word_tokenize(doc[key])[:1000] for key in doc.keys()}\n",
    "\n",
    "# Calculate TTR and its numerator / denominator\n",
    "num = {key: len(tokens[key]) for key in doc.keys()}\n",
    "den = {key: len(set(tokens[key])) for key in doc.keys()}\n",
    "ttr = {key: round(num[key]/den[key],2) for key in doc.keys()}\n",
    "\n",
    "# Print results\n",
    "print(ttr)\n",
    "print(num)\n",
    "print(den)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPXrkC/YsLxJHVPOh+p9SFa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
