{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/6.tests/PermutationTest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWi802SCcvCC"
   },
   "source": [
    "This notebook explores the use of the permutation test to assess the significance of coefficents learned in logistic regression (testing against the null that each $\\beta$ = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO0UOYCLcvCG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from random import choices\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-GgtiqJc5xo"
   },
   "outputs": [],
   "source": [
    "# get LMRD data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/train.tsv -O lmrd_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/dev.tsv -O lmrd_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/test.tsv -O lmrd_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwhM7f38c6aj"
   },
   "outputs": [],
   "source": [
    "# get Convote data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/train.tsv -O convote_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/dev.tsv -O convote_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/test.tsv -O convote_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dALCXYcTc6qn"
   },
   "outputs": [],
   "source": [
    "# get LoC data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/train.tsv -O loc_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/dev.tsv -O loc_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/test.tsv -O loc_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zqzfRM2cvCI"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            # assumes text is already tokenized\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ibpydHUcvCI"
   },
   "outputs": [],
   "source": [
    "# Change this to the name of the dataset to properly read in files below.\n",
    "data=\"convote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYfy-nFOcvCJ"
   },
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s_train.tsv\" % data)\n",
    "devX, devY=read_data(\"%s_dev.tsv\" % data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agiLpsKacvCJ"
   },
   "outputs": [],
   "source": [
    "def featurize(trainX, devX):\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(trainX)\n",
    "    X_dev = vectorizer.transform(devX)\n",
    "\n",
    "    return X_train, X_dev, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTsL3wxNcvCK"
   },
   "outputs": [],
   "source": [
    "def train(X_train, trainY, le):\n",
    "    Y_train=le.transform(trainY)\n",
    "    logreg = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    return logreg\n",
    "    return logreg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H68lmfITcvCK"
   },
   "outputs": [],
   "source": [
    "def test(logreg, devX_feat, devY, le):\n",
    "    Y_dev=le.transform(devY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_feat, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnJBf-mncvCL"
   },
   "outputs": [],
   "source": [
    "def analyze_weights(coefs, label_encoder, vocab, p_values):\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    sort_index = np.argsort(coefs)\n",
    "\n",
    "    print(label_encoder.inverse_transform([0])[0])\n",
    "    for k in sort_index[:25]:\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))\n",
    "\n",
    "    print(label_encoder.inverse_transform([1])[0])\n",
    "\n",
    "    for k in reversed(sort_index[-25:]):\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRdNJnrBcvCL"
   },
   "outputs": [],
   "source": [
    "X_train, X_dev, vectorizer=featurize(trainX, devX)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "\n",
    "logreg=train(X_train, trainY, le)\n",
    "test(logreg, X_dev, devY, le)\n",
    "\n",
    "true_coefficients=logreg.coef_[0]\n",
    "\n",
    "# We'll set P=100 here to finish running in class, but set higher (e.g., 10000) for real applications\n",
    "P=100\n",
    "\n",
    "p_values=np.zeros(len(true_coefficients))\n",
    "permutedY=copy.deepcopy(trainY)\n",
    "\n",
    "for i in range(P):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "    # permute the values of Y so that they're now attached to random data points in X\n",
    "    shuffle(permutedY)\n",
    "\n",
    "    # train logistic regression on that permuted dataset\n",
    "    permuted_logreg=train(X_train, permutedY, le)\n",
    "    coefficients=permuted_logreg.coef_[0]\n",
    "\n",
    "    # test how often the coefficients learned from the permuted data are as extreme as\n",
    "    # the coefficients from the true data\n",
    "    for idx, coef in enumerate(coefficients):\n",
    "        if abs(true_coefficients[idx]) < abs(coef):\n",
    "            p_values[idx]+=1./P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPN8pvtUcvCM"
   },
   "outputs": [],
   "source": [
    "inverse_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "out=open(\"weights.txt\", \"w\")\n",
    "for idx, coef in enumerate(true_coefficients):\n",
    "    out.write(\"%.3f\\t%s\\t%.5f\\n\" % (coef, inverse_vocab[idx], p_values[idx]))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Y1mnqKIcvCN"
   },
   "outputs": [],
   "source": [
    "analyze_weights(true_coefficients, le, vectorizer.vocabulary_, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAbdLdQscvCN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
