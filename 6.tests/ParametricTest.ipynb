{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/6.tests/ParametricTest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecvY5ezp0wb"
   },
   "source": [
    "This notebook explores a simple hypothesis test checking whether the accuracy of a trained model for binary classification is meaningfully different from a majority class baseline.  We test this making a parametric assumption: we assume that the binary correct/incorrect results follow a binomial distribution (and approximate the binomial with a normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlggnu6sp0wf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ1fXpp9qJhH"
   },
   "source": [
    "Download whichever data you used for the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bip-kY1qCJL"
   },
   "outputs": [],
   "source": [
    "# get LMRD data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/train.tsv -O lmrd_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/dev.tsv -O lmrd_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/lmrd/test.tsv -O lmrd_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5EERsnLqC30"
   },
   "outputs": [],
   "source": [
    "# get Convote data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/train.tsv -O convote_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/dev.tsv -O convote_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/convote/test.tsv -O convote_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78_qd4S1qEnQ"
   },
   "outputs": [],
   "source": [
    "# get LoC data\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/train.tsv -O loc_train.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/dev.tsv -O loc_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp24/refs/heads/main/data/loc/test.tsv -O loc_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbhcE652p0wh"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQJaI0plp0wi"
   },
   "outputs": [],
   "source": [
    "# Change this to the name of the dataset to properly read in files below.\n",
    "data=\"convote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiI9m7s9p0wi"
   },
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s_train.tsv\" % data)\n",
    "devX, devY=read_data(\"%s_dev.tsv\" % data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ldeIi_Ip0wi"
   },
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority=labelCounts.most_common(1)[0][0]\n",
    "\n",
    "    correct=0.\n",
    "    for label in devY:\n",
    "        if label == majority:\n",
    "            correct+=1\n",
    "\n",
    "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))\n",
    "    return correct/len(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooOU4CqSp0wj"
   },
   "outputs": [],
   "source": [
    "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
    "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
    "repub_dictionary=set([\"growth\",\"economy\"])\n",
    "\n",
    "def political_dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in dem_dictionary:\n",
    "            feats[\"word_in_dem_dictionary\"]=1\n",
    "        if word in repub_dictionary:\n",
    "            feats[\"word_in_repub_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRW4BbxEp0wj"
   },
   "outputs": [],
   "source": [
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqj7nqiKp0wk"
   },
   "outputs": [],
   "source": [
    "def build_features(trainX, feature_functions):\n",
    "    data=[]\n",
    "    for doc in trainX:\n",
    "        feats={}\n",
    "\n",
    "        # sample text data is already tokenized; if yours is not, do so here\n",
    "        tokens=doc.split(\" \")\n",
    "\n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xysSq9v0p0wk"
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids\n",
    "def create_vocab(data):\n",
    "    feature_vocab={}\n",
    "    idx=0\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat]=idx\n",
    "                idx+=1\n",
    "\n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNpeVKHJp0wk"
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation\n",
    "# that we can fit in a scikit-learn model.  This is important because almost all feature\n",
    "# values will be 0 for most documents (note: why?), and we don't want to save them all in\n",
    "# memory.\n",
    "\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZZi5OD2p0wl"
   },
   "outputs": [],
   "source": [
    "# This function evaluates a list of feature functions on the training/dev data arguments\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwlbtFPhp0wl"
   },
   "outputs": [],
   "source": [
    "# This function trains a model and returns the predicted and true labels for test data\n",
    "def evaluate(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    predictions=logreg.predict(devX_ids)\n",
    "    return (predictions, devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjF1Vn8Hp0wl"
   },
   "outputs": [],
   "source": [
    "baseline=majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNP_3CzXp0wl"
   },
   "outputs": [],
   "source": [
    "def binomial_test(predictions, truth, baseline, significance_level=0.95):\n",
    "    correct=[]\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        correct.append(int(pred==gold))\n",
    "\n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # two-tailed test\n",
    "    critical_value=(1-significance_level)/2\n",
    "    # ppf finds z such that p(X < z) = critical_value\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    print(\"Critical value: %.3f\\tz_alpha: %.3f\" % (critical_value, z_alpha))\n",
    "\n",
    "    # the standard error is the square root of the variance/sample size\n",
    "    # the variance for a binomial test is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    Z=(success_rate-baseline)/standard_error\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    pval=norm.cdf(-abs(Z))\n",
    "    print (\"Accuracy: %.3f, n = %s\" % (success_rate, len(correct)))\n",
    "    print(\"%s%% Confidence interval: [%.3f,%.3f]\" % (significance_level*100, lower, upper))\n",
    "\n",
    "    print(\"Z score: %.3f\" % Z)\n",
    "    print(\"p-value: %.5f\" % pval)\n",
    "\n",
    "    print (\"Critical region corresponding to z_alpha=[%.3f,%.3f]: [%.3f, %.3f]\" % (-z_alpha, z_alpha, baseline-z_alpha*standard_error, baseline+z_alpha*standard_error))\n",
    "    print (\"Can we reject null that %.3f is different from %.3f at %s significance level? %s\" % (success_rate, baseline, significance_level*100, \"Yes\" if Z < -z_alpha or Z > z_alpha else \"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvNTs2DGp0wl"
   },
   "outputs": [],
   "source": [
    "features=[political_dictionary_feature]\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_kmP1TLp0wm"
   },
   "outputs": [],
   "source": [
    "features=[unigram_feature]\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw9WEO7Lp0wm"
   },
   "source": [
    "**TODO**: Now apply this same method to the models you just submitted for the last homework (i.e., by copying the feature definitions from your homework here).  Are the features you created significantly better than a majority class baseline? Is one significantly better than the other?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8bt2oV5p0wm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
